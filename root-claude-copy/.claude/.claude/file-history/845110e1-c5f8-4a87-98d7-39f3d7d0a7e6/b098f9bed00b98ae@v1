# Plan: Scale `/analyze-git-contributions` for Large Repositories

## Problem Statement

The `/analyze-git-contributions` skill encounters token limit errors when analyzing repositories with many commits (e.g., 1,379 commits):

**Error Example:**
```
File content (89,049 tokens) exceeds maximum allowed tokens (25,000)
```

**Root Cause:**
- `collect-user-commits.sh` returns all commits (1,379 lines)
- Agent runs `extract-commit-details.sh` for each commit
- Combined output stored in tool result file exceeds Read tool's 25K token limit
- Agent cannot process the results

**Requirements:**
1. Handle repositories with 500-10,000+ commits without hitting token limits
2. Maintain meaningful AI analysis and component grouping
3. Provide user control over scope and sampling strategy
4. Keep skill responsive and usable

## Solution Strategy: Multi-Tier Approach

### Core Principle (from reading-logs skill)
**"Count first, filter/sample, then read"** - Never process all data without checking volume first

### Tier 1: Count and Decide (<500 commits)
- Process all commits with full details
- Current implementation works fine

### Tier 2: User Choice (500-2000 commits)
- Count commits and present options to user
- Options: date range filter, intelligent sampling, or full analysis
- Default: intelligent sampling

### Tier 3: Mandatory Filtering (>2000 commits)
- Require date range or sampling
- Too many commits for effective AI grouping anyway
- Guide user to focus on specific time periods

## Implementation Approach

### Phase 1: Add Commit Counting Script

**New script:** `scripts/count-user-commits.sh`

```bash
#!/bin/bash
# Fast commit count without fetching details
# Usage: ./count-user-commits.sh <repo_path> <author_pattern>
# Output: Single number (commit count)
# Exit codes: 0=success, 1=invalid repo, 2=no commits

git -C "$REPO_PATH" log --author="$AUTHOR_PATTERN" --all --oneline | wc -l
```

**Why:** Instant feedback on repository size before collecting data

### Phase 2: Add Intelligent Sampling Script

**New script:** `scripts/sample-commits.sh`

```bash
#!/bin/bash
# Samples commits using time-stratified strategy
# Usage: ./sample-commits.sh <repo_path> <author_pattern> <sample_size>
# Strategy: Divide timeline into buckets, sample evenly from each
# Output: Same format as collect-user-commits.sh (subset of commits)

# Algorithm:
# 1. Get date range (first and last commit dates)
# 2. Divide into N time buckets (e.g., 10 buckets for 200 samples)
# 3. Sample ~sample_size/N commits from each bucket
# 4. Ensures coverage across entire contribution timeline
```

**Why:**
- Maintains temporal diversity (captures work from entire timeline)
- More representative than "first N" or "random N" commits
- Useful for portfolios, annual reviews, long-running projects

### Phase 3: Add Date Range Filtering Script

**Enhancement:** Modify `collect-user-commits.sh` to support optional date filters

```bash
# Add optional parameters:
# $3: --since date (optional)
# $4: --until date (optional)

if [ -n "$3" ]; then
    SINCE_FLAG="--since=$3"
fi
if [ -n "$4" ]; then
    UNTIL_FLAG="--until=$4"
fi

git -C "$REPO_PATH" log \
    --author="$AUTHOR_PATTERN" \
    --all \
    $SINCE_FLAG \
    $UNTIL_FLAG \
    --date=iso \
    --format='%H|%an|%ae|%ad|%s'
```

**Why:**
- Most users want recent work analysis (last 6 months, last year)
- Natural way to scope the analysis
- git native filtering (fast)

### Phase 4: Update SKILL.md Workflow

**New Phase 2 Logic (Data Collection):**

```markdown
### Phase 2: Data Collection

#### Step 1: Count commits
1. Run `scripts/count-user-commits.sh` to get total commit count
2. Store count for decision logic

#### Step 2: Decide strategy based on count

**If count < 500:**
- Proceed with full analysis (existing workflow)
- Run `collect-user-commits.sh` to gather all commits
- For each commit, run `extract-commit-details.sh`

**If count 500-2000:**
- Present user with options using AskUserQuestion:
  1. **Intelligent sampling (Recommended)** - Analyze 200 representative commits
  2. Date range filter - Specify time period (e.g., last 6 months)
  3. Full analysis - Process all commits (may take longer)

**If count > 2000:**
- Inform user: "Found 3,450 commits. Please narrow the scope:"
- Options:
  1. Date range filter (required)
  2. Intelligent sampling with date range
- Do not offer "full analysis" option (would hit token limits)

#### Step 3: Collect based on strategy

**For full analysis:**
- Use existing `collect-user-commits.sh`
- Process all commit details

**For intelligent sampling:**
- Run `scripts/sample-commits.sh <repo> <author> 200`
- Get evenly distributed sample across timeline
- Process sampled commit details
- Note in report: "Analysis based on 200 representative commits"

**For date range filter:**
- Run `collect-user-commits.sh <repo> <author> <since> <until>`
- Process filtered commits
- Note in report: "Analysis for period: YYYY-MM-DD to YYYY-MM-DD"

#### Step 4: Token budget check (safety)
- Before processing commit details, estimate token usage
- Rule of thumb: ~65 tokens per commit on average
- If estimated tokens > 20,000, reduce sample size or warn user
- This prevents edge cases (commits with huge diffs)
```

### Phase 5: Enhance Report Metadata

**Add analysis scope information:**

```markdown
# Git Contributions Analysis
**Repository:** /path/to/repo
**Author:** Sean Reed
**Analysis Scope:** 200 sampled commits (out of 1,379 total)
**Sampling Strategy:** Time-stratified (evenly distributed across contribution timeline)
**Date Range:** 2024-01-15 to 2026-01-25
**Total Commits Analyzed:** 200
```

**For date-filtered:**
```markdown
**Analysis Scope:** Commits from 2025-06-01 to 2026-01-25
**Total Commits in Period:** 342
**Total Commits (All Time):** 1,379
```

## Token Budget Analysis

### Current State (Broken)
- 1,379 commits × ~65 tokens/commit = ~89,600 tokens
- Exceeds 25,000 token Read limit

### With Sampling (200 commits)
- 200 commits × 65 tokens/commit = ~13,000 tokens
- Well under 25,000 token limit
- Leaves ~12,000 tokens for AI analysis and context

### With Date Filter (6 months, ~300 commits)
- 300 commits × 65 tokens/commit = ~19,500 tokens
- Under 25,000 token limit
- Leaves ~5,500 tokens for analysis

### Safety Margin
- Target: Keep commit details under 15,000 tokens
- Maximum sample size: ~230 commits
- Recommended default: 200 commits

## Sampling Strategy Details

### Time-Stratified Sampling

**Algorithm:**
1. Get first and last commit dates
2. Divide timeline into 10 equal buckets
3. Sample 20 commits from each bucket (20 × 10 = 200 total)
4. If bucket has <20 commits, take all
5. Within each bucket, sample evenly distributed commits

**Example (1,379 commits over 2 years):**
```
Bucket 1: 2024-01 to 2024-03 (20 commits sampled from 150 total)
Bucket 2: 2024-04 to 2024-06 (20 commits sampled from 140 total)
...
Bucket 10: 2025-11 to 2026-01 (20 commits sampled from 130 total)
```

**Benefits:**
- Captures early work (project setup, initial features)
- Captures middle period (main development)
- Captures recent work (current focus)
- Representative of entire contribution arc
- Better than "most recent N" for portfolio/review purposes

### Alternative: Recent-Weighted Sampling

**For ongoing projects where recent work is more important:**
- 50% of samples from last 6 months
- 30% from 6-12 months ago
- 20% from >12 months ago

**When to use:** User indicates they want to focus on recent work

## User Experience Flow

### Scenario 1: Small Repo (<500 commits)
```
User: /analyze-git-contributions /path/to/repo
Agent: Found 342 commits for Sean Reed. Analyzing all commits...
[Generates full analysis]
```

### Scenario 2: Medium Repo (500-2000 commits)
```
User: /analyze-git-contributions /path/to/repo
Agent: Found 1,379 commits for Sean Reed.

       How would you like to proceed?

       1. Intelligent sampling (Recommended) - Analyze 200 representative
          commits evenly distributed across your contribution timeline

       2. Date range filter - Specify a time period
          (e.g., last 6 months: --since='2025-07-01')

       3. Full analysis - Process all 1,379 commits
          (may take longer)

User: [Selects option 1]
Agent: Analyzing 200 sampled commits...
[Generates analysis with sampling note]
```

### Scenario 3: Large Repo (>2000 commits)
```
User: /analyze-git-contributions /path/to/repo
Agent: Found 3,450 commits for Sean Reed.

       To provide focused analysis, please narrow the scope:

       1. Recent work - Last 6 months (Recommended)
       2. Recent work - Last year
       3. Custom date range - Specify dates
       4. Intelligent sampling - 200 commits across entire timeline

User: [Selects option]
Agent: [Proceeds with filtered/sampled analysis]
```

## Files to Modify/Create

### New Files
1. `~/.claude/skills/analyze-git-contributions/scripts/count-user-commits.sh`
   - Fast commit counting
   - ~10 lines of bash

2. `~/.claude/skills/analyze-git-contributions/scripts/sample-commits.sh`
   - Time-stratified sampling implementation
   - ~80-100 lines of bash
   - Uses git log with date filtering and head/tail

### Modified Files
1. `~/.claude/skills/analyze-git-contributions/scripts/collect-user-commits.sh`
   - Add optional `--since` and `--until` parameters
   - ~5 line change

2. `~/.claude/skills/analyze-git-contributions/SKILL.md`
   - Update Phase 2 (Data Collection) with new logic
   - Add token budget awareness
   - Add sampling strategy documentation
   - Update example outputs to show scope metadata
   - ~100 lines of changes/additions

## Implementation Steps

### Step 1: Create count-user-commits.sh
- Simple wrapper around `git log --oneline | wc -l`
- Test with target repo (should return 1379)

### Step 2: Create sample-commits.sh
- Implement time-stratified sampling algorithm
- Test with various sample sizes (50, 100, 200)
- Verify even distribution across timeline

### Step 3: Enhance collect-user-commits.sh
- Add optional date range parameters
- Maintain backward compatibility (existing usage still works)
- Test with date filters

### Step 4: Update SKILL.md
- Rewrite Phase 2 with new decision logic
- Add user interaction patterns for each scenario
- Update examples with scope metadata
- Add token budget guidelines

### Step 5: Test end-to-end
- Test with small repo (<500 commits) - should work as before
- Test with medium repo (1,379 commits) - should offer options
- Test with sampling - verify token usage stays under limit
- Test with date filtering - verify correct commits selected

## Testing Plan

### Test Case 1: Small Repo
- Repository: Any with <500 commits
- Expected: Full analysis without prompting
- Verify: All commits processed

### Test Case 2: Medium Repo (Target)
- Repository: `/Users/seanreed/PythonProjects/audivi/other/realtime_experiment`
- Commits: 1,379
- Expected: Prompt user for strategy
- Test sampling: Should return 200 commits, evenly distributed
- Verify: Token count < 25,000

### Test Case 3: Date Range Filter
- Repository: Same as above
- Filter: Last 6 months
- Expected: ~300-400 commits
- Verify: All commits within date range

### Test Case 4: Token Safety
- Create artificial test with commits having large diffs
- Verify: Token estimation catches overflow before Read tool fails
- Expected: Agent reduces sample size or warns user

## Success Criteria

- ✅ Handles repositories with 500-10,000+ commits without token errors
- ✅ Provides user control over analysis scope
- ✅ Maintains meaningful AI analysis quality
- ✅ Backward compatible (small repos work as before)
- ✅ Clear user communication about what was analyzed
- ✅ Sampling provides representative coverage across timeline
- ✅ Token usage stays under 20,000 for commit details

## Edge Cases

### Very Recent Author (Few Commits)
- <50 commits: Always do full analysis
- Don't offer sampling for small datasets

### Sparse Contributions
- Author has 500 commits over 5 years but very sparse
- Sampling still works (may get 1-2 commits per bucket)
- Report notes sparse contribution pattern

### Bursty Contributions
- Author made 1000 commits in 1 month, then nothing
- Time stratification handles this (samples from active period)
- Better than pure random sampling

### Merge Commits
- Some commits may be merges with huge diffs
- Token estimation accounts for this (conservative estimate)
- If single commit exceeds budget, skip details for that commit

## Future Enhancements (Out of Scope)

### Progressive Summarization
- For very large repos, do two-tier analysis
- First: Group commits using lightweight metadata only
- Second: Fetch full details only for representative commits from each group

### Interactive Refinement
- Generate initial analysis from sample
- Offer to "drill into" specific components with more commits
- User-guided iterative exploration

### Caching
- Cache commit details locally
- Speed up re-analysis with different scopes
- Useful for experimentation

## Critical Files

### To Create
- `~/.claude/skills/analyze-git-contributions/scripts/count-user-commits.sh`
- `~/.claude/skills/analyze-git-contributions/scripts/sample-commits.sh`

### To Modify
- `~/.claude/skills/analyze-git-contributions/scripts/collect-user-commits.sh`
- `~/.claude/skills/analyze-git-contributions/SKILL.md`

### Reference
- `~/.claude/skills/reading-logs/SKILL.md` - "Filter first" principle
- Current implementation handles token limits similarly to log file analysis
